{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredcuellv/Proyecto-Final-IA-2023/blob/main/Proyecto_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LIBRERÍAS\n",
        "\"\"\"\n",
        "\n",
        "from pandas.io.formats.style import Subset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn import decomposition\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, train_test_split, KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, f1_score, matthews_corrcoef\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.utils.validation import column_or_1d"
      ],
      "metadata": {
        "id": "vjbwf7mNbEI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srFC3SgSHZ27",
        "outputId": "14de4dee-df8e-4b2c-d165-090adc7a7357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9adc8f63ffe1>:9: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,21,22,23,25,26,27,28,29,30,31,32,35,36,37,38,39,42,47,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataCultivos = pd.read_csv('NIRSdata.csv', delimiter= ',', encoding='ISO-8859-1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.29      18.52       0.49      ...  0.5149937  0.521809   0.527157 ]\n",
            " [ 1.29      18.52       0.49      ...  0.5277381  0.5347335  0.540491 ]\n",
            " [ 1.69      25.43       0.74      ...  0.5714682  0.5784513  0.5842493]\n",
            " ...\n",
            " [ 1.22      17.18       0.66      ...  0.4490449  0.4571437  0.4630668]\n",
            " [ 7.79       2.51       0.15      ...  0.5050989  0.5098143  0.5134151]\n",
            " [ 4.31      21.3        0.39      ...  0.4862949  0.4918318  0.4951547]]\n",
            "[[ 0.]\n",
            " [ 0.]\n",
            " [ 0.]\n",
            " ...\n",
            " [95.]\n",
            " [95.]\n",
            " [95.]]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "EXTRACCION Y LIMPIEZA DE DATOS\n",
        "\"\"\"\n",
        "\n",
        "#Se sabe de antelacion que las etiquetas seran los cultivos y las caracteristicas\n",
        "#seran las columnas con valores numericos del data set (No todas)\n",
        "\n",
        "#Extraer los datos\n",
        "dataCultivos = pd.read_csv('NIRSdata.csv', delimiter= ',', encoding='ISO-8859-1')\n",
        "\n",
        "#Eliminar las filas donde las etiquetas tengan valores NaN\n",
        "dataCultivos.dropna(subset=['Cultivo'], inplace=True)\n",
        "\n",
        "#Eliminar otros valores no deseados en las etiquetas\n",
        "dataCultivos = dataCultivos[~dataCultivos['Cultivo'].isin(['0', 'Pendiente', 'No Indica'])]\n",
        "\n",
        "#Definir caracteristicas (Solo se toman las que tienen valores numericos por lo pronto)\n",
        "numeric_columns = dataCultivos.select_dtypes(include=[float, int]).columns\n",
        "X = dataCultivos[numeric_columns]\n",
        "\n",
        "#Eliminar las columnas que sabemos no seran de utilidad\n",
        "X = X.drop(['Unnamed: 0', '...2.x', '...2.y'],axis = 1)\n",
        "\n",
        "#Verificar el numero de NaN para cada caracteristica\n",
        "num_nan_por_columna = X.isna().sum(axis=0)\n",
        "#Con esto verificamos que caracteristicas tienen muchos NaN para posterior eliminacion\n",
        "#Tambien verificar que otras caracteristicas tiene algunos NaN llenarlos por metodos de imputacion\n",
        "#print(\"La variable X tiene los siguientes valores NaN por columna:\")\n",
        "#print(num_nan_por_columna)\n",
        "\n",
        "#Para las caracteristicas que suponene un alto numero de valores NaN se eliminan\n",
        "X = X.drop(['Capacidad de intercambio catiónico (CICA) acetato de amonio','% humedad gravimetrica',\n",
        "            'Porcentaje de arcilla (% Ar)', 'Porcentaje de arena (% A)', 'Porcentaje de limo (% L)'], axis = 1)\n",
        "\n",
        "#Mostrar los nombres de los cultivos sin repetir\n",
        "#print(dataCultivos['Cultivo'].unique())\n",
        "\n",
        "#Mapeo de etiquetas\n",
        "#Pasar de letras a numeros las etiquetas\n",
        "cultivos = dataCultivos['Cultivo'].unique()\n",
        "#print(len(cultivos))\n",
        "dict_cultivos = {etiqueta: valor for valor, etiqueta in enumerate(cultivos)}\n",
        "\n",
        "#Definir etiquetas(Aun no esta limpia del todo)\n",
        "Y = dataCultivos['Cultivo'].map(dict_cultivos)\n",
        "\n",
        "#Para los que tienen pocos valores NaN, se pueden deducir por otros metodos.\n",
        "#Se aplicara KNN_Imputer\n",
        "\n",
        "#Hacer una matriz general de caracteristicas y etiquetas \n",
        "#Concatenar Etiquetas con caracteristicas\n",
        "#print(X.shape)\n",
        "#print(Y.shape)\n",
        "Y = Y.to_numpy()\n",
        "Y = Y.reshape(3176,1)\n",
        "YX = np.concatenate((Y, X), axis=1)\n",
        "#print(YX.shape)\n",
        "cultivos_num = np.arange(0,len(cultivos)) #Creamos un array que tenga los cultivos, nos servira como indexador\n",
        "columnas = len(cultivos_num) #Esto es para saber cuantas matrices quedaran\n",
        "matrices = [] #Inicializar la lista de matrices\n",
        "num_cultivoX = 0 #Esta variable sirve para crear matrices auxiliares que iran llenando la lista\n",
        "cont = 0\n",
        "for i in range(0,columnas):\n",
        "    cultivo = cultivos_num[i]\n",
        "    for z in range(0,len(YX)):\n",
        "      if YX[z,0] == cultivo:\n",
        "        num_cultivoX += 1\n",
        "    #print(\"Cantidad de cultivo \", cultivo, \"en data set efectivo = \", num_cultivoX)\n",
        "    matriz_j = np.zeros((num_cultivoX, 252))\n",
        "    num_cultivoX = 0\n",
        "    for j in range(0, len(YX)):\n",
        "        etiqueta = np.array(YX[j, 0])\n",
        "        caracteristicas = np.array(YX[j, 1:])\n",
        "        #Aca se hace el llenado de la matriz de una misma etiqueta\n",
        "        if etiqueta == cultivo:\n",
        "          etiqueta = etiqueta.reshape(1, 1)\n",
        "          caracteristicas = caracteristicas.reshape(1,251)\n",
        "          matriz_j[cont] = np.concatenate((etiqueta,caracteristicas), axis = 1)\n",
        "          cont += 1\n",
        "    cont = 0\n",
        "    matrices.append(matriz_j)\n",
        "\n",
        "#LLenar valores NaN\n",
        "from sklearn.impute import KNNImputer\n",
        "import math \n",
        "#Thumb rule para definir los k vecinos, ya que al ser de diferentes tamaños cada matriz, deberia de\n",
        "#tener tambien diferentes k\n",
        "def k_ThumbRule(n):\n",
        "    raiz = math.floor(math.sqrt(n))\n",
        "    if raiz % 2 == 0:\n",
        "        raiz -= 1\n",
        "    return raiz\n",
        "def matriz_contiene_nan(matriz):\n",
        "    return np.isnan(matriz).any()\n",
        "j_elem = 0\n",
        "incom = np.zeros((3,), dtype=int)\n",
        "\"\"\"\n",
        "Debido a que hay cultivos que tienen solo una muestra, estos tendran caracteristicas\n",
        "incompletas, mas adelante se eliminaran estas, pues se consideran outliners al solo\n",
        "tener una muestra\n",
        "\"\"\"\n",
        "for i in range(0,len(matrices)):\n",
        "  #print(len(matrices[i]))\n",
        "  k = k_ThumbRule(len(matrices[i]))\n",
        "  imputer = KNNImputer(n_neighbors=k)\n",
        "  #print(matrices[i].shape)\n",
        "  matrices[i] = imputer.fit_transform(matrices[i])\n",
        "  #print(matrices[i].shape)\n",
        "  #Aca se verifica que cultivos quedaron incompletos y se guarda su indice para posterior eliminacion\n",
        "  if len(matrices[i][0]) != 252:\n",
        "    #print(\"Matriz de cultivo:\",i,\"con caracteristicas incompletas\")\n",
        "    incom[j_elem] = i\n",
        "    #print(incom[j_elem])\n",
        "    j_elem += 1\n",
        "    #print(matrices[i].shape)\n",
        "  #if matriz_contiene_nan(matrices[i]):\n",
        "    #print(\"La matriz\",i, \"contiene valores NaN.\")\n",
        "  #else:\n",
        "    #print(\"La matriz\",i, \"no contiene valores NaN.\")\n",
        "\n",
        "#Elminar cultivos que solo tienen una muestra\n",
        "for i in range(0,j_elem):\n",
        "  matriz_eliminar = incom[i] - i\n",
        "  matrices.pop(matriz_eliminar)\n",
        "\n",
        "#Crear matriz final, ya limpia y definir las etiquetas y caracteristicas limpias\n",
        "matriz_final = np.vstack(matrices)\n",
        "#np.random.shuffle(matriz_final)\n",
        "Y_limpia = matriz_final[:,0]\n",
        "Y_limpiaT = Y_limpia.T\n",
        "Y_limpiaT = Y_limpiaT.reshape(3173,1)\n",
        "X_limpia = matriz_final[:,1:]\n",
        "\n",
        "print(X_limpia)\n",
        "print(Y_limpiaT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "REDUCCIÓN DIMENSIONAL POR PCA\n",
        "\"\"\"\n",
        "\n",
        "X = X_limpia\n",
        "Y = Y_limpiaT\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "\n",
        "# Obtener la varianza explicada acumulativa\n",
        "pca = PCA()\n",
        "pca.fit(scaled_data)\n",
        "varianza_explicada_acumulativa = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Encontrar el mínimo número de componentes que cumplan el criterio del 99% de varianza explicada\n",
        "n = np.argmax(varianza_explicada_acumulativa >= 0.99) + 1\n",
        "\n",
        "# Realizar PCA con el número mínimo de componentes seleccionado\n",
        "#como se redujo un monton las caracteristicas, se lo pueden agregar un par mas a n\n",
        "n += 3\n",
        "pca = PCA(n_components=n)\n",
        "X_transformado = pca.fit_transform(scaled_data)\n",
        "\n",
        "print(\"Número de componentes seleccionados:\", n)\n",
        "print(\"Pesos de PCA:\",pca.explained_variance_ratio_)\n",
        "\n",
        "X = X_transformado\n",
        "y = Y.reshape(3173,)"
      ],
      "metadata": {
        "id": "MzDUKQR0O01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ad2c16-d705-4925-fd08-f26d443159d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de componentes seleccionados: 10\n",
            "Pesos de PCA: [0.79395679 0.14642609 0.02917655 0.00788464 0.00556766 0.00465292\n",
            " 0.0033867  0.00293992 0.00234339 0.00160493]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por SVM"
      ],
      "metadata": {
        "id": "_HWgI0h1a5Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SVM:\n",
        "Se usara maquinas de soporte vectorial, implementando cross validation para la separación de los datos\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#Empezaremos con maquinas de soporte vectorial con diferentes kernel\n",
        "#Estos kernels deberian ser adecuados tomando en cuenta la dimensionalidad\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba\n",
        "# Probar con varias semillas\n",
        "import random\n",
        "\n",
        "def crear_vector_aleatorio(n):\n",
        "    vector = []\n",
        "    for _ in range(n):\n",
        "        valor = random.randint(0, 100)\n",
        "        vector.append(valor)\n",
        "    return vector\n",
        "\n",
        "Nr = 10  #Numero de semillas\n",
        "r = crear_vector_aleatorio(Nr)\n",
        "\n",
        "for i in range(0,Nr):\n",
        "  # Creacion de un objeto KFold para dividir los datos en k pliegues\n",
        "  kfold = KFold(n_splits=5, shuffle=True, random_state=r[i])\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  # Creacion del modelo SVM con kernel RBF\n",
        "  svm_model = SVC(kernel='rbf')\n",
        "\n",
        "  #Se realiza cross validation\n",
        "  for train_index, test_index in kfold.split(X):\n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index]\n",
        "      # Entrenamiento del modelo con conjuntos de entrenamiento\n",
        "      svm_model.fit(X_train, y_train)\n",
        "      y_pred.extend(svm_model.predict(X_test))\n",
        "      y_true.extend(y_test)\n",
        "\n",
        "\n",
        "  #Evaluar con matthews\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  print('Coeficiente de Matthews:', mcc)\n",
        "\n",
        "  #Evaluar con F1 Score\n",
        "  f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "  print(\"Puntuación F1:\", f1)\n"
      ],
      "metadata": {
        "id": "jxkxNbYjO9G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed67f2b0-38fc-4ee3-99dc-f6d9204d6d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coeficiente de Matthews: 0.29158569063822326\n",
            "Puntuación F1: 0.32576079649628076\n",
            "Coeficiente de Matthews: 0.2847069615224632\n",
            "Puntuación F1: 0.31943418628675013\n",
            "Coeficiente de Matthews: 0.29038644587228496\n",
            "Puntuación F1: 0.32586602538677883\n",
            "Coeficiente de Matthews: 0.29755700197362434\n",
            "Puntuación F1: 0.3307569303791116\n",
            "Coeficiente de Matthews: 0.2918393957553591\n",
            "Puntuación F1: 0.32614048136790624\n",
            "Coeficiente de Matthews: 0.2893959555235474\n",
            "Puntuación F1: 0.3233646430144131\n",
            "Coeficiente de Matthews: 0.2924612389565957\n",
            "Puntuación F1: 0.32506534559521943\n",
            "Coeficiente de Matthews: 0.29038644587228496\n",
            "Puntuación F1: 0.32586602538677883\n",
            "Coeficiente de Matthews: 0.2999186426226635\n",
            "Puntuación F1: 0.33116364908828166\n",
            "Coeficiente de Matthews: 0.2925184158215588\n",
            "Puntuación F1: 0.325902816262563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por ANN\n",
        "\n"
      ],
      "metadata": {
        "id": "PILbizyyazTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\"\"\"\n",
        "GRIDSEARCH para ANN\n",
        "Se aplica busqueda por rejilla para obtener el número de capas ocultas de las redes neuronales\n",
        "\"\"\"\n",
        "\n",
        "# Define los valores para el número de capas ocultas\n",
        "hidden_layers = [i for i in range(10, 50)]\n",
        "\n",
        "# Crea el modelo de red neuronal\n",
        "model = MLPClassifier()\n",
        "\n",
        "# Crea el diccionario de parámetros para la búsqueda de cuadrícula\n",
        "param_grid = {'hidden_layer_sizes': hidden_layers}\n",
        "\n",
        "# Realiza la búsqueda de cuadrícula\n",
        "y = column_or_1d(y, warn=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Muestra los resultados de la búsqueda de cuadrícula\n",
        "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "print(\"Mejor puntuación:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZRcYSaugbek",
        "outputId": "23fad0d3-c371-4f7a-93ee-0fd2d3ca7a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ANN:\n",
        "Se usara redes neuronales, implementando cross validation para la separación de los datos\n",
        "y diferentes funciones de activación\n",
        "\"\"\"\n",
        "\n",
        "def metricas(mlp, X, y, cv):\n",
        "  # Calcular métricas\n",
        "  y_pred = cross_val_predict(mlp, X, y, cv=5)\n",
        "  f1 = f1_score(y, y_pred, average='weighted')\n",
        "  matthews = matthews_corrcoef(y, y_pred)\n",
        "\n",
        "  # Imprimir los resultados de la validación cruzada y las métricas\n",
        "  print(\"Precisión promedio:\", scores.mean())\n",
        "  print(\"Precisión por fold:\", scores)\n",
        "  print(\"F1 score:\", f1)\n",
        "  print(\"Coeficiente de Matthews:\", matthews)\n",
        "\n",
        "# Crear un clasificador de Red Neuronal Artificial con función de activación tanh\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(45,), activation='tanh', learning_rate='adaptive', max_iter=3000)\n",
        "\n",
        "# Realizar validación cruzada (con 5 folds)\n",
        "y = column_or_1d(y, warn=True)\n",
        "cv=5\n",
        "scores = cross_val_score(mlp, X, y, cv=5, scoring='accuracy')\n",
        "metricas(mlp, X, y, cv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91lYscuQc8QM",
        "outputId": "cc2b74bd-f7c5-466d-b3a4-be4b8aba8a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión promedio: 0.5594078342730818\n",
            "Precisión por fold: [0.57480315 0.55275591 0.5496063  0.56466877 0.55520505]\n",
            "F1 score: 0.5398711753105824\n",
            "Coeficiente de Matthews: 0.47560160521954037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un clasificador de Red Neuronal Artificial con función de activación relu\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(25,), activation='relu', max_iter=3000)\n",
        "\n",
        "# Realizar validación cruzada (con 5 folds)\n",
        "y = column_or_1d(y, warn=True)\n",
        "scores = cross_val_score(mlp, X, y, cv=5, scoring='accuracy')\n",
        "metricas(mlp, X, y, cv)\n",
        "\n",
        "\n",
        "# Crear un clasificador de Red Neuronal Artificial con función de activación logistic\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(25,), activation='logistic', max_iter=3000)\n",
        "\n",
        "# Realizar validación cruzada (con 5 folds)\n",
        "y = column_or_1d(y, warn=True)\n",
        "scores = cross_val_score(mlp, X, y, cv=5, scoring='accuracy')\n",
        "metricas(mlp, X, y, cv)"
      ],
      "metadata": {
        "id": "kpffSLsL_4Y6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}